{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the afuthors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 1 - WS 2023 -->\n",
    "\n",
    "# Regression in all Shapes and Sizes (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the first assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some of the simplest neural networks possible. \n",
    "Essentially, these simple networks come down to a well-known tool in statistics: **regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nnumpy import data\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# visual functions\n",
    "\n",
    "\n",
    "def show_1d_model(x, y, w, b, pred_func, err_func):\n",
    "    \"\"\"Visualise 1D model on data\"\"\"\n",
    "    # visualise data\n",
    "    plt.scatter(x, y, color='steelblue')\n",
    "    \n",
    "    # compute error\n",
    "    logits = my_first_network(x, w, b)\n",
    "    pred = pred_func(logits)\n",
    "    err = np.mean(err_func(pred, y), axis=0).item()\n",
    "    \n",
    "    # plot network as function\n",
    "    _x = np.linspace(x.min(), x.max())[:, None]\n",
    "    _s = my_first_network(_x, w, b)\n",
    "    _pred = pred_func(_s)\n",
    "    plt.plot(_x, _pred, color='tomato', linewidth=3, \n",
    "             label='prediction (err = {:.4f})'.format(err))\n",
    "    \n",
    "    # prettification\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def show_2d_model(x, y, w, b, pred_func, err_func, cmap='viridis'):\n",
    "    \"\"\"Visualise 2D model on data\"\"\"\n",
    "    # evaluate network in grid of points for contours\n",
    "    _x = np.linspace(x.min(), x.max())\n",
    "    _x1, _x2 = np.meshgrid(_x, _x)\n",
    "    _x = np.c_[_x1.flat, _x2.flat]\n",
    "    _s = my_first_network(_x, w, b)\n",
    "    _pred = pred_func(_s)\n",
    "    if _pred.shape[-1] > 1:\n",
    "        # use softmax-weighted avg of labels for plotting\n",
    "        _pred *= np.arange(_pred.shape[-1])\n",
    "        _pred = np.sum(_pred, axis=-1)\n",
    "    \n",
    "    # visualise data and contours\n",
    "    _pred = _pred.reshape(_x1.shape)\n",
    "    vmin = min(_pred.min(), y.min())\n",
    "    vmax = max(_pred.max(), y.max())\n",
    "    plt.scatter(*x.T, c=np.squeeze(y), cmap=cmap, vmin=vmin, vmax=vmax, edgecolors='w')\n",
    "    plt.colorbar().set_label('$y$')\n",
    "    plt.contour(_x1, _x2, _pred, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # compute error\n",
    "    logits = my_first_network(x, w, b)\n",
    "    pred = pred_func(logits)\n",
    "    if pred.shape[-1] > 1:\n",
    "        # compute error on one-hot values\n",
    "        y = to_one_hot(y, pred.shape[-1])\n",
    "    err = np.mean(err_func(pred, y), axis=0).item()\n",
    "    \n",
    "    # prettification\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.legend(handles=[\n",
    "        plt.Line2D([], [], color='black', \n",
    "                   label='prediction contours (err = {:.4f})'.format(err))\n",
    "    ], loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a natural choice for modelling linear relationships in statistics. Ater all, it maximises the likelihood of a linear model if the noise in the data is assumed to be Gaussian. On top of that, the *Maximum Likelihood Estimator* can be written down analytically. Since this model corresponds to a single-layer neural network without activation function, it is a good starting point for exploring neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model\n",
    "\n",
    "Concretely, the outputs $\\boldsymbol{y} \\in \\mathbb{R}^K$ are assumed to depend on the inputs $\\boldsymbol{x} \\in \\mathbb{R}^D$ and random noise $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ so that\n",
    "\n",
    "$$\\boldsymbol{y} = f(\\boldsymbol{x}) + \\boldsymbol{\\varepsilon},$$\n",
    "\n",
    "where $f : \\mathbb{R}^D \\to \\mathbb{R}^K$ is some linear function. \n",
    "To model the underlying relationship $f$, we will use one of the simplest possible *Neural Networks*:\n",
    "\n",
    "$$g(\\boldsymbol{x} \\mathbin{;} \\theta) = \\boldsymbol{w} \\cdot \\boldsymbol{x} + \\boldsymbol{b},$$\n",
    "\n",
    "where $\\theta = \\{\\boldsymbol{w}, \\boldsymbol{b}\\}$ is the set of parameters for the model.\n",
    "**Note** that this network corresponds to a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Likelihood\n",
    "\n",
    "The *Likelihood* of a supervised machine learning model for a dataset with \n",
    "*identically and independenltly distributed* inputs $\\boldsymbol{X}$ and outputs $\\boldsymbol{Y}$ is given by\n",
    "\n",
    "$$\\mathcal{L}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y}) = \\prod_{n = 1}^{N} p(\\boldsymbol{x}^n, \\boldsymbol{y}^n \\mathbin{;} \\theta) = \\prod_{n = 1}^{N} p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta).$$\n",
    "\n",
    "This likelihood quantises how likely some data is, given the parameters of a model.\n",
    "Given the model assumptions as stated above, the conditional probablity \n",
    "in the likelihood product can be written as \n",
    "\n",
    "$$p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) = p_\\mathcal{N}\\left(\\boldsymbol{y}^n \\mathbin{;} g(\\boldsymbol{x}^n \\mathbin{;} \\theta), \\sigma^2\\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(\\frac{-\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2}{2 \\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Optimisation\n",
    "\n",
    "Maximising the likelihood, $\\mathcal{L}(\\theta)$, is equivalent to maximising the *log-likelihood*, $\\mathcal{l}(\\theta) = \\ln \\mathcal{L}(\\theta)$, or minimising the additive inverse. Therefore, the optimal parameters, $\\theta^*$ are specified by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\theta^* & = \\arg\\min_\\theta \\left\\{-\\mathcal{l}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y}) \\right\\} \\\\\n",
    "  & = \\arg\\min_\\theta \\left\\{-\\sum_{n = 1}^{N} \\ln p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) \\right\\} \\\\\n",
    "  & = \\arg\\min_\\theta \\left\\{N \\ln \\sqrt{2 \\pi \\sigma^2} + \\frac{1}{2 \\sigma^2} \\sum_{n = 1}^{N}\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2 \\right\\},\n",
    "\\end{aligned}$$\n",
    "\n",
    "which is equivalent to minimising the sum of squared errors\n",
    "\n",
    "$$\\theta^* = \\arg\\min_\\theta \\left\\{\\frac{1}{2} \\sum_{n = 1}^{N}\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Numpy Regression (2 Points)\n",
    "\n",
    "The model described above should be straightforward to implement. How about you get familiar with [python](https://docs.python.org/3) and [numpy](https://docs.scipy.org/doc/numpy) (again)?\n",
    "\n",
    "> Implement the simple neural network from above as well as the squared error loss, using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5083d519f20566718c78c85f6eba7fb0",
     "grade": false,
     "grade_id": "cell-6255a64d4389490a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_first_network(x, w, b):\n",
    "    \"\"\"\n",
    "    Predict a value for some input with a simple neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        The input samples to predict a value for.\n",
    "    w : (K, D) ndarray\n",
    "        The input parameters for the neural network.\n",
    "    b : (K, ) ndarray\n",
    "        The bias parameter for the neural network.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prediction : (N, K) ndarray\n",
    "        The predicted value for the input from the network.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def squared_error(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the squared error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, K) ndarray\n",
    "        The squared error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e4abca2199f58041023a7b471dd521f",
     "grade": true,
     "grade_id": "cell-18f31d79f68903b8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x, y = data.gen_linear_data(num_dimensions=3, seed=1856)\n",
    "w = rng.normal(0, 1, (y.shape[1], x.shape[1]))\n",
    "b = rng.normal(0, 1, y.shape[1])\n",
    "out = my_first_network(x, w, b)\n",
    "assert isinstance(out, np.ndarray), (\n",
    "    \"ex1: output of my_first_network is not a numpy array\"\n",
    ")\n",
    "assert out.shape == y.shape, (\n",
    "    \"ex1: output of my_first_network has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3de3cd87fabed3bfab35b80227f8ee40",
     "grade": true,
     "grade_id": "cell-3dca636d77441809",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "errors = squared_error(np.zeros_like(y), y)\n",
    "assert isinstance(errors, np.ndarray), (\n",
    "    \"ex1: output of squared_error is not a numpy array\"\n",
    ")\n",
    "assert errors.shape == y.shape, (\n",
    "    \"ex1: output of squared_error has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# visualisation (with random weights and bias)\n",
    "x, y = data.gen_linear_data(num_dimensions=1, seed=1856)\n",
    "w = rng.normal(0, 1, (y.shape[1], x.shape[1]))\n",
    "b = rng.normal(0, 1, y.shape[1])\n",
    "\n",
    "show_1d_model(x, y, w, b, lambda x: x, squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Analytical Solution (2 Points)\n",
    "\n",
    "Minimising the negative log-likelihood comes down to solving linear regression with the *least-squares* method, i.e. by minimising the sum of squared residuals. For the least-squares method in linear regression, the optimal parameters can be derived analytically. The optimal solution is given by:\n",
    "\n",
    "$$\\theta^* = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{Y}.$$\n",
    "\n",
    "Does this model incorporate a bias parameter? Where can the bias parameter be found?\n",
    "\n",
    "> Implement the `analytical_solution` function so that it computes this analytical solution for given inputs and outputs. \n",
    "\n",
    "**Hint:** to get help for commands, execute a cell with following code (e.g. for help on `np.linalg.inv`):\n",
    "```python\n",
    "?np.linalg.inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25fef091a5117520d23ea2c2988e2ce7",
     "grade": false,
     "grade_id": "cell-72dccdd4abf4cce6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analytical_solution(x, y):\n",
    "    \"\"\"\n",
    "    Get the optimal parameters for linear regression,\n",
    "    given input data and target values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Input data to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Target values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : (K, D) ndarray\n",
    "        The optimal input parameters for the network.\n",
    "    b : (K, ) ndarray\n",
    "        The optimal bias parameters for the network\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9ee5631083189b053e48330e52ee348",
     "grade": true,
     "grade_id": "cell-98ef652df62dae22",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x, y = data.gen_linear_data(num_dimensions=3, centre=(2, 3), seed=1856)\n",
    "w, b = analytical_solution(x, y)\n",
    "assert isinstance(w, np.ndarray), (\n",
    "    \"ex2: w output of analytical_solution is not a numpy array\"\n",
    ")\n",
    "assert w.shape == (y.shape[1], x.shape[1]), (\n",
    "    \"ex2: w output of analytical_solution has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b38b6c9570043cafb2963e8fbf515d04",
     "grade": true,
     "grade_id": "cell-9f728dc51fd7f7b1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert isinstance(b, np.ndarray), (\n",
    "    \"ex2: bias of analytical_solution is not a numpy array\"\n",
    ")\n",
    "assert b.shape == (w.shape[0], ), (\n",
    "    \"ex2: bias of analytical_solution has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# visualisation\n",
    "x, y = data.gen_linear_data(num_dimensions=1, centre=(1, 0), seed=1856)\n",
    "w, b = analytical_solution(x, y)\n",
    "\n",
    "show_1d_model(x, y, w, b, lambda x: x, squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The values to predict are not always continuous. A very common task in machine learning is to answer yes-no questions, e.g. Is this an image of a cat? These problems are known as *binary classification* tasks. This can be encoded by labelling positive examples with a 1 and negative examples with a 0. As a result, the target values are Bernoulli distributed, rather than Gaussian. This also means that the relationship is no longer linear and linear regression makes little sense. This is where *Logistic Regression* comes in play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model\n",
    "\n",
    "Mathematically, we assume that the relation between inputs and outputs can be modelled as follows:\n",
    "\n",
    "$$\\boldsymbol{y} = \\begin{cases}\n",
    "  1 & f(\\boldsymbol{x}) > 0.5 \\\\\n",
    "  0 & f(\\boldsymbol{x}) \\leq 0.5\n",
    "\\end{cases}.$$\n",
    "\n",
    "Since the relation is non-linear, we add an additional non-linearity, $\\sigma : \\mathbb{R} \\to \\mathbb{R}$, to our simple network so that\n",
    "\n",
    "$$g(\\boldsymbol{x} \\mathbin{;} \\theta) = \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x} + \\boldsymbol{b}\\right),$$\n",
    "\n",
    "where the parameters are again $\\theta = \\{\\boldsymbol{w}, \\boldsymbol{b}\\}$. The non-linear function maps real values to values between zero and one. This allows the outputs of the network to be interpreted as the probability for its input to be classified as positive. For logistic regression, this non-linearity is the logistic sigmoid, which is defined as \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_sigmoid(s):\n",
    "    \"\"\"\n",
    "    Compute the logistic sigmoid function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : ndarray\n",
    "        The logits to apply the logistic sigmoid function on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : ndarray\n",
    "        The probabilitie(s) for the given logit(s).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Logistic Likelihood Maximisation (1 Point)\n",
    "\n",
    "\n",
    "Just as for linear regression, maximum likelihood estimation can be used to find parameters for a model that solves the binary classification problem. Is there an error function for logistic regression, like we have the squared error for linear regression? \n",
    "\n",
    "The derivation of the objective for logistic regression is similar to that of the squared error in the context of linear regression. \n",
    "Feel free to use your [$\\LaTeX$](https://www.overleaf.com/learn) skills to complete the derivation below:\n",
    "\n",
    "Again, we start from the conditional probability of the label:\n",
    "$$p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) = p_\\mathrm{B}\\left(\\boldsymbol{y}^n \\mathbin{;} g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right) = g(\\boldsymbol{x}^n \\mathbin{;} \\theta)^{\\boldsymbol{y}^n} (1 - g(\\boldsymbol{x}^n \\mathbin{;} \\theta))^{1 - \\boldsymbol{y}^n}.$$\n",
    "\n",
    "The negative log-likelihood is then given by:\n",
    "$$\\begin{aligned}\n",
    "  -\\mathcal{l}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y})\n",
    "  & = -\\sum_{n = 1}^{N} \\ln p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Hint**: the probability mass function of the Bernoulli distribution, $p_\\mathrm{B}(k \\mathbin{;} p) = \\begin{cases} 1 - p & k = 0 \\\\ p & k = 1 \\end{cases}$, can be written as $p_\\mathrm{B}(k \\mathbin{;} p) = p^k (1 - p)^{1 - k}$\n",
    "\n",
    "\n",
    "> Implement the resulting *logistic error* in the `logistic_error` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Notes on LaTeX in Jupyter Notebooks\n",
    "\n",
    "$\\LaTeX$ is an advanced typesetting system that can be used for all sorts of documents. One of the key features of $\\LaTeX$ is the ability to insert mathematical formulas in the text. Jupyter notebooks allow to display mathematical symbols (and some other things) through $\\LaTeX$ syntax. Technically the conversion from $\\LaTeX$ code to HTML is done by [MathJax](https://www.mathjax.org/), so if you cannot see the nice formulas I put in this notebook, you might need to troubleshoot MathJax.\n",
    "\n",
    "To give you an idea of the possibilities of $\\LaTeX$:\n",
    "\n",
    "| code                   | rendered             | code (big)               | rendered (big)         |\n",
    "|:---------------------- | --------------------:|:------------------------ | ----------------------:|\n",
    "| `$\\LaTeX$`             | $\\LaTeX$             |                          |                        |\n",
    "| `$\\ln \\boldsymbol{x}$` | $\\ln \\boldsymbol{x}$ | `$$\\ln \\boldsymbol{x}$$` | $$\\ln \\boldsymbol{x}$$ |\n",
    "| `$\\frac{a}{b}$`        | $\\frac{a}{b}$        | `$$\\frac{a}{b}$$`        | $$\\frac{a}{b}$$        |\n",
    "| `$\\sum_{i=1}^I i^2$`   | $\\sum_{i=1}^I i^2$   | `$$\\sum_{i=1}^I i^2$$`   | $$\\sum_{i=1}^I i^2$$   |\n",
    "\n",
    "If you have troubles to find the right symbols, you can use [detexify](http://detexify.kirelabs.org/classify.html) or directly use a more graphical $\\LaTeX$ equation editor, e.g. [this one from codecogs](https://www.codecogs.com/latex/eqneditor.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50827dd9e8fb8eec1568f9c0bcee316b",
     "grade": false,
     "grade_id": "cell-53b40cf7f1176fc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_error(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the logistic error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, K) ndarray\n",
    "        The logistic error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3606f88962b4eb5104572a160132e5f2",
     "grade": true,
     "grade_id": "cell-7f166a87eef667ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "_, y = data.gen_blob_data(num_dimensions=3, seed=1856)\n",
    "preds = np.linspace(0.01, 0.99, len(y))[:, None]\n",
    "errors = logistic_error(preds, y)\n",
    "assert isinstance(errors, np.ndarray), (\n",
    "    \"ex3: output of logistic_error is not a numpy array\"\n",
    ")\n",
    "assert errors.shape == y.shape, (\n",
    "    \"ex3: output of logistic_error has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# visualisation (with random weights and bias)\n",
    "x, y = data.gen_blob_data(num_dimensions=1, seed=1856)\n",
    "w, b = rng.normal(0, 1, (1, x.shape[1])), rng.normal(0, 1, 1)\n",
    "\n",
    "show_1d_model(x, y, w, b, logistic_sigmoid, logistic_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient Descent (2 Points)\n",
    "\n",
    "Since there is no closed form solution for logistic regression, other methods are necessary to find the optimal parameters. Gradient descent is one of the simplest methods to get to a good solution. By pushing the parameters in the opposite direction of the gradient, it is possible to find local minima relatively fast.\n",
    "\n",
    "Concretely, the gradient descent algorithm computes the gradients for some inputs and moves a certain amount in the direction of that gradient. This process is repeated a number of times until convergence. The amount of gradient to go down is known as the *learning rate*, and a single step of computing gradients and updating parameters is commonly called an *epoch*.\n",
    "\n",
    "> Compute the derivatives for the parameters in logistic regression. Implement the `logistic_grad_func` so that it returns the derivatives of the logistic error w.r.t. the parameters. Make sure not to forget any parameters in the gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "850699bfc35c1b213cca20c14b4d79b4",
     "grade": false,
     "grade_id": "cell-61fd03d6a89754c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(grad_func, x, y, bias=True, epochs=10, lr=1.):\n",
    "    \"\"\"\n",
    "    Use gradient descent to compute the weights of a network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grad_func : callable\n",
    "        Function that computes the error and gradients for the model.\n",
    "        Signature should correspond to `err, grads = step_func(x, y, w, b)`.\n",
    "    x : ndarray\n",
    "        Inputs to the network.\n",
    "    y : ndarray\n",
    "        Targets for the network.\n",
    "    bias : bool, optional\n",
    "        Whether or not the network should have a bias term.\n",
    "    epochs : int, optional\n",
    "        Number of steps to descent.\n",
    "    lr : float, optional\n",
    "        Scaling factor for gradient steps.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray\n",
    "        The optimal input parameters for the network.\n",
    "    b : ndarray, optional\n",
    "        The optimal bias parameters for the network.\n",
    "        Only returned if `bias=True`.\n",
    "    \"\"\"\n",
    "    # initial guess for parameters\n",
    "    w = np.zeros((y.shape[1], x.shape[1]))\n",
    "    b = np.zeros(y.shape[1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        err, (dw, db) = grad_func(x, y, w, b)\n",
    "        \n",
    "        w -= lr * dw\n",
    "        if bias:\n",
    "            b -= lr * db\n",
    "            \n",
    "        # print mean error at most 5 times during learning\n",
    "        if epochs < 5 or i % (epochs // 5) == 0:\n",
    "            avg_err = np.mean(err, axis=0).item()\n",
    "            print(\"epoch {:03d}: {:.4f}\".format(i, avg_err))\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logistic_grad_func(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic error w.r.t. the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Inputs to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Targets for the network.\n",
    "    w : (K, D) ndarray\n",
    "        Input weights of the network.\n",
    "    b : (K, ) ndarray\n",
    "        Bias weights of the network.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    err : (N, K) ndarray\n",
    "        The error of the prediction from the network \n",
    "        for the given input data before the update.\n",
    "    grads : ((K, D) ndarray, (K, ) ndarray)\n",
    "        Gradients of the logistic error function w.r.t. the parameters,\n",
    "        i.e. `dw` and `db` (in that order).\n",
    "        \n",
    "    \"\"\"\n",
    "    logits = my_first_network(x, w, b)\n",
    "    pred = logistic_sigmoid(logits)\n",
    "    err = logistic_error(pred, y)\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return err, (dw, db)\n",
    "\n",
    "\n",
    "def logistic_gradient_descent(x, y, bias=True, epochs=10, lr=1e-3):\n",
    "    return gradient_descent(logistic_grad_func, x, y, bias, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e83577b62eb36225845f82260aeb62cb",
     "grade": true,
     "grade_id": "cell-ac64392341e7fb4f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "x, y = data.gen_blob_data(num_dimensions=3, seed=1856)\n",
    "w = np.zeros((y.shape[1], x.shape[1]))\n",
    "b = np.zeros(y.shape[1])\n",
    "_, (dw, db) = logistic_grad_func(x, y, w, b)\n",
    "assert isinstance(dw, np.ndarray), (\n",
    "    \"ex4: dw output of logistic_grad_func is not a numpy array\"\n",
    ")\n",
    "assert dw.shape == w.shape, (\n",
    "    \"ex4: dw output of logistic_grad_func has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f56f716f27c71b73b8a068e8cd689e41",
     "grade": true,
     "grade_id": "cell-37a59a46a4ac9ed3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "assert isinstance(db, np.ndarray), (\n",
    "    \"ex4: db output of logistic_grad_func is not a numpy array\"\n",
    ")\n",
    "assert db.shape == b.shape, (\n",
    "    \"ex4: db output of logistic_grad_func has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# visualisation\n",
    "x, y = data.gen_blob_data(num_dimensions=1, seed=1856)\n",
    "w, b = logistic_gradient_descent(x, y, epochs=100, bias=True)\n",
    "\n",
    "show_1d_model(x, y, w, b, logistic_sigmoid, logistic_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Softmax regression is essentially a generalisation of binary logistic regression that allows to solve *multi-class problems*, i.e. questions with more than two possible (exclusive) answers. Instead of predicting a single probability, the goal in softmax regression is to predict a probability for each of the possible classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: One-hot Encoding (1 Point)\n",
    "\n",
    "Since the labels are mostly integer values, some encoding is necessary to have the targets reflect probabilities. E.g. if there are three classes, the labels would be `0`, `1`, `2`. The most common way to encode these labels as probability vectors is to use a one-hot encoding. A one-hot code uses as much bits as there are labels. The code is zero everywhere, except at the index that corresponds to the label, where it is one. E.g. in the case of the three zero-indexed labels, the encoding for the label `1` would be `[0, 1, 0]`\n",
    "\n",
    "> Implement a function to compute one-hot encodings from integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7a80a6492432afdf72402e5436d1022",
     "grade": false,
     "grade_id": "cell-dd1cb7910c3dc429",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, k=None):\n",
    "    \"\"\"\n",
    "    Compute a one-hot encoding from a vector of integer labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : (N, 1) ndarray\n",
    "        The zero-indexed integer labels to encode.\n",
    "    k : int, optional\n",
    "        The number of distinct labels in `y`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    one_hot : (N, k) ndarray\n",
    "        The one-hot encoding of the labels.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype='int')\n",
    "    if k is None:\n",
    "        k = np.amax(y) + 1\n",
    "       \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4244e6b2645ffc8df37c1255e24c2fad",
     "grade": true,
     "grade_id": "cell-70ee029208625acf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "y = np.arange(5)[:, None]\n",
    "code = to_one_hot(y)\n",
    "assert isinstance(code, np.ndarray), (\n",
    "    \"ex5: output of to_one_hot is not a numpy array\"\n",
    ")\n",
    "assert code.shape == (y.shape[0], y[-1] + 1), (\n",
    "    \"ex5: output of to_one_hot has incorrect shape\"\n",
    ")\n",
    "assert to_one_hot(y, k=10).shape == (y.shape[0], 10), (\n",
    "    \"ex5: output of to_one_hot has incorrect shape when k=10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y = rng.integers(3, size=(5, 1))\n",
    "print(\"original y:\")\n",
    "print(y.ravel())\n",
    "print(\"result:\")\n",
    "print(to_one_hot(y, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model\n",
    "\n",
    "One possible way to model multiple output classes, is to consider a function $f : \\mathbb{R}^D \\to \\mathbb{R}^K$, where $K$ is the number of labels, which allows to specify the model as follows\n",
    "\n",
    "$$y = \\arg\\max_i f_i(\\boldsymbol{x}).$$\n",
    "\n",
    "Note that every binary decision problem can be formulated as a multi-class problem with two classes. To extend the logistic sigmoid non-linearity to multiple classes, we can consider how the prediction for the negative class can be computed. Assuming the probability of the positive class is computed by:\n",
    "$$\\sigma\\left(s\\right) = \\frac{1}{1 + e^{-s}},$$\n",
    "the probability of the negative class is given by:\n",
    "$$1 - \\sigma\\left(s\\right) = \\frac{e^{-s}}{1 + e^{-s}} = \\sigma(-s).$$\n",
    "\n",
    "The non-linearity that is the result of generalising the logistic sigmoid to multiple classes,\n",
    "$$\\mathrm{softmax}_i(\\boldsymbol{x}) = \\frac{e^{x_i}}{\\sum_j e^{x_j}},$$\n",
    "is known as the *softmax* function and it can be verified that $\\mathrm{softmax}(\\,(x, 0)\\,) = (\\sigma(x), \\sigma(-x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Likelihood\n",
    "\n",
    "The Likelihood for softmax regression can be derived in a similar way as you have done it for logistic regression --- using a categorical instead of a Bernoulli distribution. Assuming that the labels, $\\boldsymbol{y}$, are given as one-hot vectors, the resulting negative log-likelihood is known as the *cross entropy* loss:\n",
    "\n",
    "$$-l(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y}) = \\sum_{n=1}^N \\boldsymbol{y}^n \\ln g(\\boldsymbol{x}^n \\mathbin{;} \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Softmax Gradients (2 Points)\n",
    "\n",
    "Time to generalise logistic gradient descent to handle multiple labels! Either you remember these functions from the lecture or you can freshen up your memory in the script. \n",
    "\n",
    "> Implement the functions necessary for training a softmax classifier with gradient descent. Make sure not to forget any parameters in the gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08e8bd6a3c718c219e7fd9a16d4b198e",
     "grade": false,
     "grade_id": "cell-c5896126f012df8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Compute the softmax function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : (N, K) ndarray\n",
    "        The logits to apply the softmax function on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : (N, K) ndarray\n",
    "        The probabilitie(s) for the given logit(s).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the logistic error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, ) ndarray\n",
    "        The logistic error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def softmax_grad_func(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross-entropy w.r.t. the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Inputs to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Targets for the network.\n",
    "    w : (K, D) ndarray\n",
    "        Input weights of the network.\n",
    "    b : (K, ) ndarray\n",
    "        Bias weights of the network.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    err : (N, ) ndarray\n",
    "        The error of the prediction from the network \n",
    "        for the given input data before the update.\n",
    "    grads : ((K, D) ndarray, (K, ) ndarray)\n",
    "        Gradients of the logistic error function w.r.t. the parameters,\n",
    "        i.e. `dw` and `db` (in that order).\n",
    "        \n",
    "    \"\"\"\n",
    "    logits = my_first_network(x, w, b)\n",
    "    pred = softmax(logits)\n",
    "    err = cross_entropy(pred, y)\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return err, (dw, db)\n",
    "\n",
    "def softmax_gradient_descent(x, y, bias=True, epochs=10, lr=1e-3):\n",
    "    return gradient_descent(softmax_grad_func, x, y, bias, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff5b52fda89a7d40215b1dc75bc0ef5d",
     "grade": true,
     "grade_id": "cell-8e926553908ff5c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "x, _ = data.gen_blob_data((10, 10, 10, 10), num_dimensions=3, seed=1856)\n",
    "y_hot = rng.choice(np.eye(4), size=x.shape[0])\n",
    "w = np.zeros((y_hot.shape[1], x.shape[1]))\n",
    "b = np.zeros(y_hot.shape[1])\n",
    "_, (dw, db) = softmax_grad_func(x, y_hot, w, b)\n",
    "assert isinstance(dw, np.ndarray), (\n",
    "    \"ex6: dw output of logistic_grad_func is not a numpy array\"\n",
    ")\n",
    "assert dw.shape == w.shape, (\n",
    "    \"ex6: dw output of logistic_grad_func has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fddcb9cef37a99b8c402539a130b937e",
     "grade": true,
     "grade_id": "cell-a9eef950f0fa2ed7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "assert isinstance(db, np.ndarray), (\n",
    "    \"ex6: db output of logistic_grad_func is not a numpy array\"\n",
    ")\n",
    "assert db.shape == b.shape, (\n",
    "    \"ex6: db output of logistic_grad_func has incorrect shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# feel free to play around with this code!\n",
    "num_samples = (50, 50)\n",
    "x, y = data.gen_blob_data(num_samples, std=1., seed=1856)\n",
    "y_hot = to_one_hot(y, k=len(num_samples))\n",
    "w, b = softmax_gradient_descent(x, y_hot, epochs=100, bias=True)\n",
    "show_2d_model(x, y, w, b, softmax, cross_entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
